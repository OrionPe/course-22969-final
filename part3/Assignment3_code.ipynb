{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Orion Peeters\n",
        "\n",
        "#### 208409565"
      ],
      "metadata": {
        "id": "2bKc9PRGR4HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/course 22969')"
      ],
      "metadata": {
        "id": "RW5Xky-pHYiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31dbb45-1215-478b-c0bb-88b484404e5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import os\n",
        "import random\n",
        "\n",
        "# I importing the architecture and tools from the original model.\n",
        "# The .py file is in the same folder in the repo, in order for the import to work please keep it that way\n",
        "from gpt_core import (\n",
        "    GPTModel,\n",
        "    BASE_CONFIG,\n",
        "    generate,\n",
        "    text_to_token_ids,\n",
        "    token_ids_to_text,\n",
        "    format_input\n",
        ")"
      ],
      "metadata": {
        "id": "E-BhRsgEHc7s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Important Prerequisite:**\n",
        "\n",
        "The SFT model weights used in this part are not included in the repository\n",
        "because they exceed GitHubs file size limits.\n",
        "\n",
        "To run this code successfully, the user must first execute the code from Part 2\n",
        "(available in this repository). Running Part 2 will train the model and save\n",
        "the required weight file locally.\n",
        "\n",
        "After the weights are generated and saved, you will be able to load them here."
      ],
      "metadata": {
        "id": "Pe6bijz6I-LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Hardware\n",
        "def load_finetuned_model():\n",
        "    \"\"\"\n",
        "    Initializes the system and loads the specific SFT weights.\n",
        "    \"\"\"\n",
        "    print(\"\\n\")\n",
        "    print(\"INITIALIZING DEBATE SYSTEM\")\n",
        "\n",
        "    # Auto-detecting device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Hardware Device: {device}\")\n",
        "\n",
        "    # Load Tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Load Model Architecture\n",
        "    print(\"Building GPT-2 Medium Architecture...\")\n",
        "    model = GPTModel(BASE_CONFIG)\n",
        "\n",
        "    # Load Fine-Tuned Weights\n",
        "    # The user will need to change the path here to his one\n",
        "    model_path = \"/content/drive/MyDrive/course 22969/gpt2/gpt2-medium355M-sft.pth\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading weights from: {model_path}\")\n",
        "        try:\n",
        "            state_diction = torch.load(model_path, map_location=device)\n",
        "\n",
        "            # Removing '_orig_mod.' prefix added by torch.compile in part 2 code I added\n",
        "            new_state_diction = {}\n",
        "            for k, v in state_diction.items():\n",
        "                if k.startswith(\"_orig_mod.\"):\n",
        "                    new_key = k.replace(\"_orig_mod.\", \"\")\n",
        "                    new_state_diction[new_key] = v\n",
        "                else:\n",
        "                    new_state_diction[k] = v\n",
        "\n",
        "            # Loading the cleaned dictionary\n",
        "            model.load_state_dict(new_state_diction)\n",
        "            print(\"Success: Model loaded.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading weights: {e}\")\n",
        "            return None, None, None\n",
        "    else:\n",
        "        print(f\"Error: File not found at {model_path}\")\n",
        "        return None, None, None\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval() # Setting to evaluation mode\n",
        "\n",
        "    return model, tokenizer, device"
      ],
      "metadata": {
        "id": "Eq7O4Eq6HhAb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DebateAgent:\n",
        "    \"\"\"\n",
        "    Represents a single debater in the arena.\n",
        "    This class wraps the GPT model with a specific 'persona' and manages\n",
        "    the generation process.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, persona, start_phrases, model, tokenizer, device):\n",
        "\n",
        "        self.name = name\n",
        "        self.persona = persona\n",
        "        self.start_phrases = start_phrases\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.last_reply = \"\" # Memory buffer to check for repeated outputs\n",
        "\n",
        "    def reply(self, history, round_num):\n",
        "        \"\"\"\n",
        "        Generates a response based on the conversation history.\n",
        "        This method implements the next mechanism:\n",
        "        1. It attempts to generate a response.\n",
        "        2. If the response is identical to the previous turn or too short, it retries.\n",
        "        3. On retries, it increases the 'temperature' (randomness) to force a new path.\n",
        "        \"\"\"\n",
        "        max_retries = 3\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            # Picking a Random Opener\n",
        "            # I strictly force the model to start with a specific phrase.\n",
        "            # This prevents \"Agreement bias\" where the model just says \"I agree.\"\n",
        "            current_opener = random.choice(self.start_phrases)\n",
        "\n",
        "            # Formating History\n",
        "            recent = history[-2:]\n",
        "            history_str = \"\\n\".join([f\"{msg['speaker']}: {msg['content']}\" for msg in recent])\n",
        "\n",
        "            # Building Prompt\n",
        "            # preventing the model from getting stuck in a cache loop.\n",
        "            entry = {\n",
        "                \"instruction\": f\"You are {self.name}. {self.persona} Do not repeat yourself. Be specific and detailed.\",\n",
        "                \"input\": f\"ROUND: {round_num} (Attempt {attempt})\\nCONTEXT:\\n{history_str}\"\n",
        "            }\n",
        "\n",
        "            full_prompt = format_input(entry) + current_opener\n",
        "\n",
        "            # Generating\n",
        "            current_temp = 0.9 + (attempt * 0.1)\n",
        "\n",
        "            input_ids = text_to_token_ids(full_prompt, self.tokenizer).to(self.device)\n",
        "\n",
        "            out_ids = generate(\n",
        "                model=self.model,\n",
        "                idx=input_ids,\n",
        "                max_new_tokens=110,\n",
        "                context_size=1024,\n",
        "                temperature=current_temp,\n",
        "                top_k=60,\n",
        "                eos_id=50256\n",
        "            )\n",
        "\n",
        "            # Decoding and Cleaning\n",
        "            full_text = token_ids_to_text(out_ids, self.tokenizer)\n",
        "\n",
        "            # Slicing off the prompt to get just the new content\n",
        "            raw_completion = full_text[len(full_prompt):].split(\"###\")[0].strip()\n",
        "\n",
        "            final_response = f\"{current_opener} {raw_completion}\"\n",
        "\n",
        "            # If the response is exactly the same as the last round, or garbage (too short),\n",
        "            # I skip 'return' and trigger the loop to try again with higher temperature.\n",
        "            if final_response == self.last_reply or len(raw_completion) < 10:\n",
        "                continue\n",
        "            else:\n",
        "                self.last_reply = final_response\n",
        "                return final_response\n",
        "\n",
        "        # Fallback - If we fail 3 times, return whatever we generated last to keep the code running.\n",
        "        return final_response"
      ],
      "metadata": {
        "id": "iWSMr1FnkLPG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DebateArena:\n",
        "    \"\"\"\n",
        "    Orchestrates the multi-agent debate simulation.\n",
        "\n",
        "    This class acts as the 'System Controller'. It is responsible for:\n",
        "    1. Managing the shared conversation history (Context Window).\n",
        "    2. Alternating turns between the two DebateAgents.\n",
        "    3. Displaying the real-time output in a readable format.\n",
        "    \"\"\"\n",
        "    def __init__(self, topic, agent_a, agent_b):\n",
        "\n",
        "        self.topic = topic\n",
        "        self.agents = [agent_a, agent_b]\n",
        "\n",
        "        # Shared Memory: This list grows as the debate progresses.\n",
        "        # Both agents read from this list to understand the context.\n",
        "        self.history = []\n",
        "        self.round_num = 0\n",
        "\n",
        "    def start_debate(self, rounds=3):\n",
        "        \"\"\"\n",
        "        Runs the main execution loop of the debate.\n",
        "        \"\"\"\n",
        "        print(f\"\\n TOPIC: '{self.topic}'\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Seed the Conversation\n",
        "        # We need a \"Moderator\" entry to give the first agent something to reply to.\n",
        "        seed = f\"The topic is: {self.topic}. {self.agents[0].name}, go.\"\n",
        "        self.history.append({\"speaker\": \"Moderator\", \"content\": seed})\n",
        "\n",
        "        # Main Game Loop\n",
        "        for r in range(rounds):\n",
        "            self.round_num += 1\n",
        "            print(f\"--- ROUND {self.round_num} ---\")\n",
        "\n",
        "            # Iterate through agents\n",
        "            for agent in self.agents:\n",
        "\n",
        "                # Generate Reply\n",
        "                response = agent.reply(self.history, self.round_num)\n",
        "\n",
        "                # Update Shared Memory\n",
        "                self.history.append({\"speaker\": agent.name, \"content\": response})\n",
        "\n",
        "                # Print Output\n",
        "                print(f\"\\nðŸ—£ï¸  {agent.name.upper()}:\")\n",
        "                print(textwrap.fill(response, width=70))\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "        print(\"=\"*60)"
      ],
      "metadata": {
        "id": "ib1r4BOsqG4n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Test Case\n",
        "\n",
        "I choose this specific topic as a test case. The user can change topic, persona and openers if he wants to try and genrate an argument regarding a different subject"
      ],
      "metadata": {
        "id": "pdOhMvir7pYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    This block sets up the simulation parameters and initiates the debate.\n",
        "    It demonstrates the core innovation of the project: 'Biased Opener Injection'.\n",
        "\n",
        "    By pre-defining the first few words of every sentence, I force the\n",
        "    underlying language model (which inherently wants to agree/be polite)\n",
        "    to argue.\n",
        "    \"\"\"\n",
        "\n",
        "    # Loading the fine-tuned system\n",
        "    model, tokenizer, device = load_finetuned_model()\n",
        "\n",
        "    if model:\n",
        "        # These are the \"keys\" to the debate.\n",
        "        elon_openers = [\n",
        "            \"I fundamentally believe that pausing AI would be a disaster because\",\n",
        "            \"The data clearly shows that AI is already saving lives by\",\n",
        "            \"If we stop progress now, we will lose the chance to\",\n",
        "            \"We cannot let fear stop us when the benefits are\",\n",
        "            \"The reality is that AI regulation will only serve to\"\n",
        "        ]\n",
        "\n",
        "        sam_openers = [\n",
        "            \"However, we must consider the catastrophic risk that\",\n",
        "            \"But the danger is that unchecked systems will\",\n",
        "            \"You are ignoring the critical fact that AI models are\",\n",
        "            \"History warns us that powerful technology without safety leads to\",\n",
        "            \"We simply cannot control a superintelligence that is\"\n",
        "        ]\n",
        "\n",
        "        # I created two distinct agents using the same underlying GPT-2 model.\n",
        "        # The 'persona' string sets the system instruction (Context).\n",
        "        # The 'start_phrases' force the output alignment.\n",
        "\n",
        "        optimist = DebateAgent(\n",
        "            name=\"Elon\",\n",
        "            persona=\"You are a Techno-Optimist. You argue that AI is safe and essential.\",\n",
        "            start_phrases=elon_openers,\n",
        "            model=model, tokenizer=tokenizer, device=device\n",
        "        )\n",
        "\n",
        "        skeptic = DebateAgent(\n",
        "            name=\"Sam\",\n",
        "            persona=\"You are an AI Safety Researcher. You argue that AI is an existential threat.\",\n",
        "            start_phrases=sam_openers,\n",
        "            model=model, tokenizer=tokenizer, device=device\n",
        "        )\n",
        "\n",
        "        # STARTING THE SIMULATION\n",
        "        arena = DebateArena(\n",
        "            topic=\"Should AI be paused?\",\n",
        "            agent_a=optimist,\n",
        "            agent_b=skeptic\n",
        "        )\n",
        "\n",
        "        arena.start_debate(rounds=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAMCzNsMHqys",
        "outputId": "9f6e7692-5d4d-403f-b540-66573029fa54"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "INITIALIZING DEBATE SYSTEM\n",
            "Hardware Device: cuda\n",
            "Building GPT-2 Medium Architecture...\n",
            "Loading weights from: /content/drive/MyDrive/course 22969/gpt2/gpt2-medium355M-sft.pth\n",
            "Success: Model loaded.\n",
            "\n",
            " TOPIC: 'Should AI be paused?'\n",
            "============================================================\n",
            "--- ROUND 1 ---\n",
            "\n",
            "ðŸ—£ï¸  ELON:\n",
            "We cannot let fear stop us when the benefits are so obvious.\n",
            "\n",
            "ðŸ—£ï¸  SAM:\n",
            "You are ignoring the critical fact that AI models are in fact an\n",
            "existential threat. Keep this in mind when trying to reach consensus.\n",
            "\n",
            "--- ROUND 2 ---\n",
            "\n",
            "ðŸ—£ï¸  ELON:\n",
            "I fundamentally believe that pausing AI would be a disaster because it\n",
            "would remove an essential capability from the world.\n",
            "\n",
            "ðŸ—£ï¸  SAM:\n",
            "We simply cannot control a superintelligence that is changing the\n",
            "rules of the game.\n",
            "\n",
            "--- ROUND 3 ---\n",
            "\n",
            "ðŸ—£ï¸  ELON:\n",
            "We cannot let fear stop us when the benefits are so enormous!\n",
            "\n",
            "ðŸ—£ï¸  SAM:\n",
            "But the danger is that unchecked systems will soon affect every aspect\n",
            "of our lives, from our daily lives to our futures.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}